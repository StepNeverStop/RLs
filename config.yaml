UnityEnvFile:
    Empty: &env /

train:
    save_frequency: 100 # save training model every `save_frequency` train steps. Should larger for off-policy algorithms, and smaller for on-policy algorithms
    # max_step_per_episode, max_train_step, max_frame_step, max_train_episode are set to sys.maxsize if default value is set to zero.
    max_step_per_episode: 2000 # max_step_per_episode per episode, if gym.env.max_episode_steps > max_step_per_episode, then PEB(Partial Episode Bootstraping), else TimeLimit.
    max_train_step: 0       # if max_train_step > 0, then training will stop when training steps > max_train_step.
    max_frame_step: 0
    max_train_episode: 0
    moving_average_episode: 100
    allow_print: true
    inference_episode: 0
    add_noise2buffer: false # add some noise data (obtain by random action) into replay buffer, in order to prevent overfitting
    add_noise2buffer_episode_interval: 10 # episode interval when adding noise into replay buffer while training
    add_noise2buffer_steps: 1000 # how many steps should be added into replay buffer
    info: None
    wandb_project: RLs
    # off-policy
    off_policy_train_interval: 1 # train policy every interval times

unity:
    train:
        real_done: true
        pre_fill_steps: 10000
    env:
        file_path: *env
        train_time_scale: 100
        width: 1028
        height: 720
        quality_level: 5
        inference_time_scale: 0
        target_frame_rate: 60
        obs_resize: true
        resize: [84, 84]
        obs_stack: true
        stack_visual_nums: 4
        obs_grayscale: false
        obs_scale: false    # change [0, 1] to [0, 255]
        reset_config: {}

gym:
    train:
        pre_fill_steps: 10000
        render: false
        render_episode: 0 # if render_episode equals 0, it will be set to sys.maxsize, otherwise it will be set to render_episode if --render-episode is not specified through cmd line.
        eval_while_train: false
        max_eval_episode: 100
        # off-policy
        off_policy_eval_interval: 0 # only for off-policy algorithms, if LARGER THAN 0, then evaluate policy every `off_policy_eval_interval` training step
        off_policy_step_eval_episodes: 100
    env:
        # 1. multiprocessing
        # 2. threading
        # 3. ray
        # 4 vector
        vector_env_type: multiprocessing
        render_mode: random_1 # first last [list] random_[num] or all.
        action_skip: false
        skip: 4
        obs_stack: false
        stack: 4
        obs_grayscale: false
        obs_resize: false
        resize: [84, 84]
        obs_scale: false
        noop: false
        noop_max: 30
        # if max_episode_steps > train->max_step_per_episode, then when reaching train->max_step_per_episode, break but Not Done
        # if max_episode_steps < train->max_step_per_episode, then when reaching max_episode_steps, break and Done
        max_episode_steps: null

buffer:
    ER: {}
    PER:
        alpha: 0.6 # priority
        beta: 0.4 # importance sampling ratio
        epsilon: 0.01
        global_v: false
    NstepER:
        n: 4
    NstepPER:
        alpha: 0.6
        beta: 0.4
        epsilon: 0.01
        global_v: false
        n: 4
    # off-policy with rnn
    EpisodeER:
        burn_in_time_step: 20
        train_time_step: 40 # null
