UnityEnvFile:
    Empty: &env /

train:
    save_frequency: 10
    max_step_per_episode: 2000 # max_step_per_episode per episode, if gym->env->max_episode_steps > max_step_per_episode, them PEB(Partial Episode Bootstraping), else TimeLimit.
    max_train_episode: 0
    max_train_step: 100000 # if max_train_step > 0, then training will stop when training steps > max_train_step.
    max_frame_step: 0
    moving_average_episode: 100
    all_learner_print: true
    inference_episode: 0
    add_noise2buffer: false # add some noise data (obtain by random action) into replay buffer, in order to prevent overfitting
    add_noise2buffer_episode_interval: 10 # episode interval when adding noise into replay buffer while training
    add_noise2buffer_steps: 1000 # how many steps should be added into replay buffer
    info: None
    wandb_project: RLs

model:
    logger2file: false

unity:
    train:
        real_done: true
        pre_fill_steps: 10000
    env:
        file_path: *env
        train_time_scale: 100
        width: 1028
        height: 720
        resize: [84, 84]
        quality_level: 5
        inference_time_scale: 0
        target_frame_rate: 60
        stack_visual_nums: 4
        reset_config: {}

gym:
    train:
        pre_fill_steps: 10000
        off_policy_eval_interval: 0 # only for off-policy algorithms, if LARGER THAN 0, then evaluate policy every `off_policy_eval_interval` training step
        off_policy_step_eval_episodes: 100
        render: false
        render_episode: 50000
        eval_while_train: false
        max_eval_episode: 100
    env:
        render_mode: random_1 # first last [list] random_[num] or all.
        action_skip: false
        skip: 4
        obs_stack: false
        stack: 4
        obs_grayscale: false
        obs_resize: false
        resize: [84, 84]
        obs_scale: false
        noop: false
        noop_max: 30
        # if max_episode_steps > train->max_step_per_episode, then when reaching train->max_step_per_episode, break but Not Done
        # if max_episode_steps < train->max_step_per_episode, then when reaching max_episode_steps, break and Done
        max_episode_steps: null

buffer:
    ER: {}
    PER:
        alpha: 0.6 # priority
        beta: 0.4 # importance sampling ratio
        epsilon: 0.01
        global_v: false
    NstepER:
        n: 4
    NstepPER:
        alpha: 0.6
        beta: 0.4
        epsilon: 0.01
        global_v: false
        n: 4
    EpisodeER:
        burn_in_time_step: 20
        train_time_step: 40 # null
