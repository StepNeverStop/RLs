policy: &policy
    max_frame_step: 999999999999999999
    max_train_episode: 999999999999999999
    save_frequency: 1000
    save2single_file: false
    gamma: 0.99
    decay_lr: false
    normalize_vector_obs: false

    obs_with_pre_action: false

    # ----- could be overrided in specific algorithms, i.e. dqn, so as to using different type of visual net, memory net.
    rep_net_params: &rep_net_params
        vector_net_params:
            h_dim: 16
            network_type: "adaptive" # rls.nn.represents.vectors
        visual_net_params:
            h_dim: 128
            network_type: "simple" # rls.nn.represents.visuals
        encoder_net_params:
            h_dim: 16
            network_type: "identity" # rls.nn.represents.encoders
        memory_net_params:
            rnn_units: 16
            network_type: "identity" # rls.nn.represents.memories
    # -----

sarl_policy: &sarl_policy
    <<: *policy
    use_curiosity: false # whether to use ICM or not
    curiosity_lr: 1.0e-3 # the learning rate for ICM
    curiosity_reward_eta: 0.01 # scale the forward loss of ICM to shape intrinsic reward. It depends on the range of reward of specific environment.
    curiosity_beta: 0.2 # weight that scale the forward loss and inverse loss of ICM

marl_policy: &marl_policy
    <<: *policy
    obs_with_pre_action: true
    obs_with_agent_id: true
    share_params: true

sarl_on_policy: &sarl_on_policy
    <<: *sarl_policy
    epochs: 4 # train multiple times per agent step
    n_time_step: 4 # n-step or rnn length
    batch_size: 64

sarl_off_policy: &sarl_off_policy
    <<: *sarl_policy
    n_time_step: 4 # n-step or rnn length
    epochs: 1 # train multiple times per agent step
    batch_size: 64
    buffer_size: 100000
    use_priority: false

marl_off_policy: &marl_off_policy
    <<: *marl_policy
    n_time_step: 4
    epochs: 1
    batch_size: 64
    buffer_size: 100000
    use_priority: false

cem:
    <<: *sarl_on_policy
    frac: 0.2
    init_var: 1
    extra_std: 1
    envs_per_popu: 5
    extra_var_last_multiplier: 0.2
    network_settings: [32, 32]

dqn: &dqn
    <<: *sarl_off_policy
    lr: 5.0e-4
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 10000
    assign_interval: 1000
    network_settings: [64, 64]
    rep_net_params:
        <<: *rep_net_params
        visual_net_params:
            h_dim: 128
            network_type: "nature"

ddqn: *dqn

averaged_dqn:
    target_k: 4
    <<: *dqn

dddqn:
    <<: *dqn
    network_settings:
        share: [64]
        v: [64]
        adv: [64]

bootstrappeddqn:
    head_num: 4
    <<: *dqn

oc:
    <<: *sarl_off_policy
    q_lr: 5.0e-3
    intra_option_lr: 5.0e-4
    termination_lr: 5.0e-4
    use_eps_greedy: false
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    ent_coff: 0.01
    boltzmann_temperature: 1.0
    use_baseline: true
    terminal_mask: true
    double_q: false
    termination_regularizer: 0.01
    init2mid_annealing_step: 10000
    options_num: 4
    assign_interval: 1000
    network_settings:
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]

ioc:
    <<: *sarl_off_policy
    q_lr: 5.0e-3
    intra_option_lr: 5.0e-4
    termination_lr: 5.0e-4
    interest_lr: 5.0e-4
    ent_coff: 0.01
    boltzmann_temperature: 1.0
    use_baseline: true
    terminal_mask: true
    double_q: false
    termination_regularizer: 0.01
    options_num: 4
    assign_interval: 1000
    network_settings:
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]
        interest: [32, 32]

aoc:
    <<: *sarl_on_policy
    options_num: 4
    dc: 0.01
    terminal_mask: false
    eps: 0.1
    epsilon: 0.2
    value_epsilon: 0.2
    pi_beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    kl_reverse: false
    kl_target: 0.02
    kl_target_cutoff: 2
    kl_target_earlystop: 4
    kl_beta: [0.7, 1.3]
    kl_alpha: 1.5
    kl_coef: 1.0
    network_settings:
        share: [32, 32]
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]

ppoc:
    <<: *sarl_on_policy
    options_num: 4
    dc: 0.01
    terminal_mask: false
    o_beta: 1.0e-3
    epsilon: 0.2
    value_epsilon: 0.2
    pi_beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    kl_reverse: false
    kl_target: 0.02
    kl_target_cutoff: 2
    kl_target_earlystop: 4
    kl_beta: [0.7, 1.3]
    kl_alpha: 1.5
    kl_coef: 1.0
    network_settings:
        share: [32, 32]
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]
        o: [32, 32]

sql:
    <<: *sarl_off_policy
    lr: 5.0e-4
    alpha: 2
    ployak: 0.995
    network_settings: [64, 64]

c51:
    <<: *sarl_off_policy
    v_min: -100
    v_max: 100
    atoms: 51
    lr: 5.0e-4
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 10000
    assign_interval: 1000
    network_settings: [256, 256]

rainbow:
    <<: *sarl_off_policy
    v_min: -100
    v_max: 100
    atoms: 51
    lr: 5.0e-4
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 10000
    assign_interval: 1000
    network_settings:
        share: [128]
        v: [128, 128]
        adv: [128, 128]

qrdqn:
    <<: *sarl_off_policy
    nums: 20
    huber_delta: 1.
    lr: 5.0e-4
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 10000
    assign_interval: 1000
    network_settings: [256, 256]

iqn:
    <<: *sarl_off_policy
    online_quantiles: 8 # quantile number of online network
    target_quantiles: 8 # quantile number of target network
    select_quantiles: 32 # quantile number for selecting actions
    quantiles_idx: 64 # trails
    huber_delta: 1. # delta for huber loss
    lr: 5.0e-4
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 10000
    assign_interval: 1000
    network_settings:
        q_net: [128, &iqn_hm 64]
        quantile: [128, *iqn_hm]
        tile: [64]

pg:
    <<: *sarl_on_policy
    lr: 5.0e-4
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            condition_sigma: false
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]

ac:
    <<: *sarl_off_policy
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            condition_sigma: false
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

a2c:
    <<: *sarl_on_policy
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    beta: 1.0e-3
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            condition_sigma: false
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

ppo:
    <<: *sarl_on_policy
    share_net: true
    epsilon: 0.2
    ent_coef: 0.005
    vf_coef: 0.5
    lr: 0.0003
    lambda_: 0.97
    actor_lr: 3.0e-4
    critic_lr: 1.0e-3
    max_grad_norm: ~

    # duel clip
    use_duel_clip: false
    duel_epsilon: 0.1

    # value function clip
    use_vclip: false
    value_epsilon: 0.2

    # kl loss
    use_kl_loss: false
    kl_reverse: false
    kl_target: 0.02
    kl_beta: [0.7, 1.3]
    kl_alpha: 1.5
    kl_coef: 1.0

    # extra loss
    use_extra_loss: false
    extra_coef: 1000.0
    kl_target_cutoff: 2

    # early stopping
    use_early_stop: true
    kl_target_earlystop: 4

    network_settings:
        share:
            continuous:
                condition_sigma: false # not recommended
                log_std_bound: [-20, 2]
                share: [64, 64]
                mu: [64, 64]
                v: [64, 64]
            discrete:
                share: [64, 64]
                logits: [64, 64]
                v: [64, 64]
        actor_continuous:
            hidden_units: [64, 64]
            condition_sigma: false # not recommended
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

npg: &npg
    <<: *sarl_on_policy
    actor_step_size: 0.5
    epsilon: 0.2
    beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    cg_iters: 10
    damping_coeff: 0.1
    critic_lr: 1.0e-3
    train_critic_iters: 10
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            condition_sigma: false
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

trpo:
    <<: *npg
    delta: 0.01
    backtrack_iters: 10
    backtrack_coeff: 0.8

dpg:
    <<: *sarl_off_policy
    use_target_action_noise: False
    noise_action: "ou"
    noise_params:
        sigma: 0.2
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

ddpg:
    <<: *sarl_off_policy
    ployak: 0.995
    use_target_action_noise: False
    noise_action: "clip_normal"
    noise_params:
        sigma: 0.2
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

td3:
    <<: *sarl_off_policy
    ployak: 0.995
    delay_num: 2
    noise_action: "clip_normal"
    noise_params:
        sigma: 0.2 # specify the variance of gaussian distribution
        noise_bound: 0.5 # specify the clipping bound of sampled noise, noise must in range of [-bound, bound]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0 # discrete_tau越小，gumbel采样的越接近one_hot，但相应的梯度也越小
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

sac:
    <<: *sarl_off_policy
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    ployak: 0.995
    use_gumbel: false
    discrete_tau: 1.0
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
            soft_clip: False
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        q: [64, 64]

sac_v:
    <<: *sarl_off_policy
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    ployak: 0.995
    use_gumbel: false
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    discrete_tau: 1.0
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
            soft_clip: False
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        q: [64, 64]
        v: [64, 64]

tac:
    <<: *sarl_off_policy
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    # maybe equal or greater than 0. We realize that the proposed method performs better when 1 ≤ q < 2 than
    # when 0 < q < 1 and q ≥ 2, in terms of stable convergence and final total average returns.
    # q -> 0, more exploration, 2<= q -> +∞, more exploitation.
    # 1 <=q < 2 is much better. Note that q=1 equals SAC.
    entropic_index: 1.5
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    ployak: 0.995
    discrete_tau: 1.0
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
            soft_clip: False
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        q: [64, 64]

maxsqn:
    <<: *sarl_off_policy
    alpha: 0.2
    beta: 0.1 # 0 <= beta < 1 when beta approaches 1 the distribution of convergence points is closer to uniform distribution means more entropy. when beta approaches 0 the final policy is more deterministic.
    use_epsilon: false
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 10000
    auto_adaption: true
    q_lr: 5.0e-4
    alpha_lr: 5.0e-4
    ployak: 0.995
    network_settings: [64, 64]

maddpg:
    <<: *marl_off_policy
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    noise_action: "ou"
    noise_params:
        sigma: 0.2
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

vdn: &vdn
    <<: *marl_off_policy
    mixer: "vdn"
    mixer_settings: {}
    lr: 5.0e-4
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    use_double: true
    init2mid_annealing_step: 10000
    assign_interval: 1000
    network_settings:
        share: [64]
        v: [64]
        adv: [64]

qmix:
    <<: *vdn
    mixer: "qmix"
    mixer_settings:
        mixing_embed_dim: 16
        hidden_units: [16]

qatten:
    <<: *vdn
    mixer: "qatten"
    mixer_settings:
        agent_own_state_size: 1 # TODO: implement this
        query_hidden_units: [64]
        query_embed_dim: 32
        key_embed_dim: 32
        head_hidden_units: [64]
        n_attention_head: 4
        constrant_hidden_units: [32]
        is_weighted: True

qtran:
    <<: *vdn
    opt_loss: 1
    nopt_min_loss: 0.1
    mixer: "qtran-base"
    mixer_settings:
        qtran_arch: "coma_critic" # "coma_critic", "qtran_paper"
        hidden_units: [64, 64]

qplex:
    <<: *vdn
    mixer: "qplex"
    mixer_settings:
        hidden_units: [64]
        is_minus_one: true
        weighted_head: true
        # adv
        num_kernel: 4
        adv_hidden_units: [64]
