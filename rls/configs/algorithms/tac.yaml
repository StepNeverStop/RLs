super_config: 'sarl_off_policy'

alpha: 0.2
auto_adaption: true
annealing: true
last_alpha: 0.01
# maybe equal or greater than 0. We realize that the proposed method performs better when 1 ≤ q < 2 than
# when 0 < q < 1 and q ≥ 2, in terms of stable convergence and final total average returns.
# q -> 0, more exploration, 2<= q -> +∞, more exploitation.
# 1 <=q < 2 is much better. Note that q=1 equals SAC.
entropic_index: 1.5
actor_lr: 5.0e-4
critic_lr: 1.0e-3
alpha_lr: 5.0e-4
polyak: 0.995
discrete_tau: 1.0
network_settings:
  actor_continuous:
    share: [ 64, 64 ]
    mu: [ 64 ]
    log_std: [ 64 ]
    soft_clip: false
    log_std_bound: [ -20, 2 ]
  actor_discrete: [ 64, 64 ]
  q: [ 64, 64 ]