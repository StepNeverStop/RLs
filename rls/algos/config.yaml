# algorithm config = general UNION on_policy/off_policy UNION specific algorithm config
# if same key happened in multiple configurations, then specific algorithm config override on_policy/off_policy override general
# priority 1: specific algorithm config
# priority 2: on_policy/off_policy
# priority 3: general

general:
    decay_lr: false
    train_time_step: 5
    tf_dtype: float32 # float32 or float64

    use_curiosity: false # whether to use ICM or not
    curiosity_lr: 1.0e-3 # the learning rate for ICM
    curiosity_reward_eta: 0.01 # scale the forward loss of ICM to shape intrinsic reward. It depends on the range of reward of specific environment.
    curiosity_loss_weight: 5 # weight that scale the gradient loss and ICM loss
    curiosity_beta: 0.2 # weight that scale the forward loss and inverse loss of ICM

    normalize_vector_obs: false
    logger2file: false
    # ----- could be overrided in specific algorithms, i.e. dqn, so as to using different type of visual net, memory net.
    vector_net_kwargs: {}
    visual_net_kwargs:
        visual_feature: 128
        network_type: simple
    encoder_net_kwargs:
        use_encoder: false
        output_dim: 16
    memory_net_kwargs:
        use_rnn: false # always false, using -r to active RNN
        rnn_units: 16
        network_type: lstm
    # -----

on_policy:
    rnn_time_step: 8

off_policy:
    train_times_per_step: 1 # train multiple times per agent step
    # PER
    use_isw: false
    # rnn
    burn_in_time_step: 4
    episode_batch_size: 32
    episode_buffer_size: 10000

hiro:
    gamma: 0.99
    intrinsic_reward_mode: os # os or cos
    ployak: 0.995
    high_scale: 1.0
    reward_scale: 1.0
    sample_g_nums: 100
    sub_goal_steps: 10
    # 指定从第几位之后开始是真实目标信息
    fn_goal_dim: 1
    high_batch_size: 128
    high_buffer_size: 100000
    low_batch_size: 128
    low_buffer_size: 100000
    high_actor_lr: 1.0e-4
    high_critic_lr: 1.0e-3
    low_actor_lr: 1.0e-4
    low_critic_lr: 1.0e-3
    network_settings:
        high_actor: [64, 64]
        high_critic: [64, 64]
        low_actor: [64, 64]
        low_critic: [64, 64]

curl:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    curl_lr: 5.0e-4
    gamma: 0.99
    ployak: 0.995
    discrete_tau: 1.0
    batch_size: 8
    buffer_size: 1000
    img_size: 64
    use_priority: false
    n_step: false
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]
        encoder: 128

cem:
    frac: 0.2
    init_var: 1
    extra_std: 1
    envs_per_popu: 5
    extra_var_last_multiplier: 0.2
    network_settings: [32, 32]

qs:
    mode: "q" # 'q', 'sarsa', 'expected_sarsa'
    lr: 0.2
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100

dqn: &dqn
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    network_settings: [64, 64]

ddqn: *dqn

dddqn:
    network_settings:
        share: [64]
        v: [64]
        adv: [64]
    <<: *dqn

bootstrappeddqn:
    head_num: 4
    <<: *dqn

oc:
    q_lr: 5.0e-3
    intra_option_lr: 5.0e-4
    termination_lr: 5.0e-4
    use_eps_greedy: false
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    ent_coff: 0.01
    boltzmann_temperature: 1.0
    use_baseline: true
    terminal_mask: true
    double_q: false
    termination_regularizer: 0.01
    init2mid_annealing_step: 100
    options_num: 4
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    network_settings:
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]

ioc:
    q_lr: 5.0e-3
    intra_option_lr: 5.0e-4
    termination_lr: 5.0e-4
    interest_lr: 5.0e-4
    gamma: 0.99
    ent_coff: 0.01
    boltzmann_temperature: 1.0
    use_baseline: true
    terminal_mask: true
    double_q: false
    termination_regularizer: 0.01
    options_num: 4
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    network_settings:
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]
        interest: [32, 32]

aoc:
    options_num: 4
    dc: 0.01
    terminal_mask: false
    eps: 0.1
    epsilon: 0.2
    value_epsilon: 0.2
    gamma: 0.99
    pi_beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    batch_size: 256
    epoch: 4 # very important
    kl_reverse: false
    kl_target: 0.02
    kl_target_cutoff: 2
    kl_target_earlystop: 4
    kl_beta: [0.7, 1.3]
    kl_alpha: 1.5
    kl_coef: 1.0
    network_settings:
        share: [32, 32]
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]

ppoc:
    options_num: 4
    dc: 0.01
    terminal_mask: false
    o_beta: 1.0e-3
    epsilon: 0.2
    value_epsilon: 0.2
    gamma: 0.99
    pi_beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    batch_size: 256
    epoch: 4 # very important
    kl_reverse: false
    kl_target: 0.02
    kl_target_cutoff: 2
    kl_target_earlystop: 4
    kl_beta: [0.7, 1.3]
    kl_alpha: 1.5
    kl_coef: 1.0
    network_settings:
        share: [32, 32]
        q: [32, 32]
        intra_option: [32, 32]
        termination: [32, 32]
        o: [32, 32]

sql:
    lr: 5.0e-4
    alpha: 2
    ployak: 0.995
    gamma: 0.99
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings: [64, 64]

c51:
    v_min: -100
    v_max: 100
    atoms: 51
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    network_settings: [256, 256]

rainbow:
    v_min: -100
    v_max: 100
    atoms: 51
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: true
    n_step: true
    network_settings:
        share: [128]
        v: [128, 128]
        adv: [128, 128]

qrdqn:
    nums: 20
    huber_delta: 1.
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    network_settings: [256, 256]

iqn:
    online_quantiles: 8 # quantile number of online network
    target_quantiles: 8 # quantile number of target network
    select_quantiles: 32 # quantile number for selecting actions
    quantiles_idx: 64 # trails
    huber_delta: 1. # delta for huber loss
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100
    batch_size: 256
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    network_settings:
        q_net: [128, &iqn_hm 64]
        quantile: [128, *iqn_hm]
        tile: [64]

pg:
    lr: 5.0e-4
    gamma: 0.99
    batch_size: 64
    condition_sigma: false
    epoch: 1 # very important
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]

ac:
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    condition_sigma: false
    gamma: 0.99
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

a2c:
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    condition_sigma: false
    gamma: 0.99
    beta: 1.0e-3
    batch_size: 64
    epoch: 4 # very important
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

ppo:
    share_net: true
    epsilon: 0.2
    gamma: 0.99
    ent_coef: 0.005
    vf_coef: 0.5
    lr: 0.0003
    lambda_: 0.97
    batch_size: 32
    policy_epoch: 4 # very important
    value_epoch: 4
    actor_lr: 3.0e-4
    critic_lr: 1.0e-3
    max_grad_norm: null
    condition_sigma: false # not recommended

    # duel clip
    use_duel_clip: false
    duel_epsilon: 0.1

    # value function clip
    use_vclip: true
    value_epsilon: 0.2

    # kl loss
    use_kl_loss: false
    kl_reverse: false
    kl_target: 0.02
    kl_beta: [0.7, 1.3]
    kl_alpha: 1.5
    kl_coef: 1.0

    # extra loss
    use_extra_loss: false
    extra_coef: 1000.0
    kl_target_cutoff: 2

    # early stopping
    use_early_stop: true
    kl_target_earlystop: 4

    network_settings:
        share:
            continuous:
                log_std_bound: [-20, 2]
                share: [64, 64]
                mu: [64, 64]
                v: [64, 64]
            discrete:
                share: [64, 64]
                logits: [64, 64]
                v: [64, 64]
        actor_continuous:
            hidden_units: [64, 64]
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

trpo:
    epsilon: 0.2
    gamma: 0.99
    beta: 1.0e-3
    lr: 5.0e-4
    delta: 0.01
    lambda_: 0.97
    cg_iters: 10
    damping_coeff: 0.1
    backtrack_iters: 10
    backtrack_coeff: 0.8
    train_v_iters: 10
    batch_size: 64
    critic_lr: 1.0e-3
    condition_sigma: false
    network_settings:
        actor_continuous:
            hidden_units: [64, 64]
            log_std_bound: [-20, 2]
        actor_discrete: [64, 64]
        critic: [64, 64]

dpg:
    gamma: 0.99
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

ddpg:
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

pd_ddpg:
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    reward_critic_lr: 1.0e-3
    cost_critic_lr: 1.0e-3
    lambda_lr: 5.0e-4
    discrete_tau: 1.0
    cost_constraint: 1.0
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        reward: [64, 64]
        cost: [64, 64]

td3:
    gamma: 0.99
    ployak: 0.995
    delay_num: 2
    noise_type: gaussian # ou or gaussian
    gaussian_noise_sigma: 0.2 # if using gaussian noise, specify the variance of gaussian distribution
    gaussian_noise_bound: 0.2 # if using gaussian noise, specify the clipping bound of sampled noise, noise must in range of [-bound, bound]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    discrete_tau: 1.0 # discrete_tau越小，gumbel采样的越接近one_hot，但相应的梯度也越小
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

sac:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    gamma: 0.99
    ployak: 0.995
    use_gumbel: true
    discrete_tau: 1.0
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]

sac_v:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    gamma: 0.99
    ployak: 0.995
    use_gumbel: true
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    discrete_tau: 1.0
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]
        v: [64, 64]

tac:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    # maybe equal or greater than 0. We realize that the proposed method performs better when 1 ≤ q < 2 than
    # when 0 < q < 1 and q ≥ 2, in terms of stable convergence and final total average returns.
    # q -> 0, more exploration, 2<= q -> +∞, more exploitation.
    # 1 <=q < 2 is much better. Note that q=1 equals SAC.
    entropic_index: 1.5
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    gamma: 0.99
    ployak: 0.995
    discrete_tau: 1.0
    batch_size: 64
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]

maxsqn:
    alpha: 0.2
    beta: 0.1 # 0 <= beta < 1 when beta approaches 1 the distribution of convergence points is closer to uniform distribution means more entropy. when beta approaches 0 the final policy is more deterministic.
    use_epsilon: false
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_step: 100
    auto_adaption: true
    q_lr: 5.0e-4
    alpha_lr: 5.0e-4
    gamma: 0.999
    ployak: 0.995
    batch_size: 256
    buffer_size: 100000
    use_priority: false
    n_step: true
    network_settings: [64, 64]

maddpg:
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    batch_size: 32
    buffer_size: 100000
    network_settings:
        actor: [32, 32]
        q: [32, 32]
