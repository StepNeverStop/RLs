general:
    tf_dtype: float32       # float32 or float64
    use_curiosity: false     # whether to use ICM or not
    curiosity_lr: 1.0e-3    # the learning rate for ICM
    curiosity_reward_eta: 1000  # scale the forward loss of ICM to shape intrinsic reward. It depends on the range of reward of specific environment.
    curiosity_beta: 0.2         # weight that scale the forward loss and inverse loss of ICM
    curiosity_loss_weight: 5    # weight that scale the gradient loass and ICM loss

dqn:    &dqn
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    hidden_units: [64, 64]

ddqn: *dqn

drqn:   &drqn
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 100
    buffer_size: 10000
    assign_interval: 1000
    hidden_units: 
        lstm: 8
        dense: [8]
    
drdqn: *drqn

dddqn:
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    hidden_units:
        share: [64]
        v: [64]
        adv: [64]

c51:
    v_min: -100
    v_max: 100
    atoms: 51
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    hidden_units: [256, 256]

rainbow:
    v_min: -100
    v_max: 100
    atoms: 51
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 100000
    assign_interval: 1000
    use_priority: true
    n_step: true
    hidden_units:
        share: [128]
        v: [128, 128]
        adv: [128, 128]

qrdqn:
    nums: 20
    huber_delta: 1.
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    hidden_units: [256, 256]

iqn:
    online_quantiles: 8 # quantile number of online network 
    target_quantiles: 8 # quantile number of target network 
    select_quantiles: 32    # quantile number for selecting actions
    quantiles_idx: 64   # trails
    huber_delta: 1. # delta for huber loss
    lr: 5.0e-4
    gamma: 0.99
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    batch_size: 1024
    buffer_size: 100000
    assign_interval: 1000
    use_priority: false
    n_step: true
    hidden_units: 
        q_net: [128, &iqn_hm 64]
        quantile: [128, *iqn_hm]
        tile: [64]

pg:
    lr: 5.0e-4
    gamma: 0.99
    batch_size: 1024
    epoch: 1 # very important
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]

ac:
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    gamma: 0.99
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

a2c:
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    gamma: 0.99
    beta: 1.0e-3
    batch_size: 1024
    epoch: 4 # very important
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

ppo:
    share_net: true
    epsilon: 0.2
    gamma: 0.99
    beta: 1.0e-3
    lr: 5.0e-4
    lambda_: 0.97
    batch_size: 64
    epoch: 4 # very important
    actor_lr: 3.0e-4
    critic_lr: 1.0e-3
    hidden_units:
        share:
            continuous:
                share: [64, 64]
                mu: [64, 64]
                v: [64, 64]
            discrete: 
                share: [64, 64]
                logits: [64, 64]
                v: [64, 64]
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

trpo:
    epsilon: 0.2
    gamma: 0.99
    beta: 1.0e-3
    lr: 5.0e-4
    delta: 0.01
    lambda_: 0.97
    cg_iters: 10
    damping_coeff: 0.1
    backtrack_iters: 10
    backtrack_coeff: 0.8
    train_v_iters: 10
    batch_size: 64
    critic_lr: 1.0e-3
    share_visual_net: true
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        critic: [64, 64]

dpg:
    gamma: 0.99
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

ddpg:
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

td3:
    gamma: 0.99
    ployak: 0.995
    delay_num: 2
    noise_type: gaussian # ou or gaussian
    gaussian_noise_sigma: 0.2 # if using gaussian noise, specify the variance of gaussian distribution
    gaussian_noise_bound: 0.2 # if using gaussian noise, specify the clipping bound of sampled noise, noise must in range of [-bound, bound]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    discrete_tau: 1.0 # discrete_tau越小，gumbel采样的越接近one_hot，但相应的梯度也越小
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous: [64, 64]
        actor_discrete: [64, 64]
        q: [64, 64]

sac:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    share_visual_net: true
    gamma: 0.99
    ployak: 0.995
    use_gumbel: true
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]

sac_v:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    gamma: 0.99
    ployak: 0.995
    use_gumbel: true
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    share_visual_net: true
    discrete_tau: 1.0
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]
        v: [64, 64]

tac:
    alpha: 0.2
    auto_adaption: true
    annealing: true
    last_alpha: 0.01
    log_std_bound: [-20, 2]
    share_visual_net: true
    # maybe equal or greater than 0. We realize that the proposed method performs better when 1 ≤ q < 2 than 
    # when 0 < q < 1 and q ≥ 2, in terms of stable convergence and final total average returns.
    # q -> 0, more exploration, 2<= q -> +∞, more exploitation.
    # 1 <=q < 2 is much better. Note that q=1 equals SAC.
    entropic_index: 1.5 
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    alpha_lr: 5.0e-4
    gamma: 0.99
    ployak: 0.995
    discrete_tau: 1.0
    batch_size: 64
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units:
        actor_continuous:
            share: [64, 64]
            mu: [64]
            log_std: [64]
        actor_discrete: [64, 64]
        q: [64, 64]

maxsqn:
    alpha: 0.2
    beta: 0.1 # 0 <= beta < 1 when beta approaches 1 the distribution of convergence points is closer to uniform distribution means more entropy. when beta approaches 0 the final policy is more deterministic.
    use_epsilon: false
    eps_init: 1
    eps_mid: 0.2
    eps_final: 0.01
    init2mid_annealing_episode: 10
    auto_adaption: true
    q_lr: 5.0e-4
    alpha_lr: 5.0e-4
    gamma: 0.999
    ployak: 0.995
    batch_size: 1024
    buffer_size: 100000
    use_priority: false
    n_step: true
    hidden_units: [64, 64]

ma_dpg:
    gamma: 0.99
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    hidden_units:
        actor: [64, 64]
        q: [64, 64]

ma_ddpg:
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    hidden_units:
        actor: [64, 64]
        q: [64, 64]

ma_td3:
    gamma: 0.99
    ployak: 0.995
    actor_lr: 5.0e-4
    critic_lr: 1.0e-3
    share_visual_net: true
    hidden_units:
        actor: [64, 64]
        q: [64, 64]
